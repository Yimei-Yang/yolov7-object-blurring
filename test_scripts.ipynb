{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QthQYc8NdLM4",
        "outputId": "f57e90d1-8c80-4e3c-8367-911acd9fb370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov7-object-blurring' already exists and is not an empty directory.\n",
            "/content/yolov7-object-blurring\n",
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt install python3.10-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: /content/yolov7-object-blurring/yolov7objblurring/bin/python3\n",
            "\n",
            "/content/yolov7-object-blurring/yolov7objblurring\n",
            "[Errno 2] No such file or directory: 'Scripts'\n",
            "/content/yolov7-object-blurring/yolov7objblurring\n",
            "/bin/bash: line 1: activate: command not found\n",
            "/content/yolov7-object-blurring\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RizwanMunawar/yolov7-object-blurring.git\n",
        "%cd yolov7-object-blurring\n",
        "\n",
        "!python3 -m venv yolov7objblurring\n",
        "%cd yolov7objblurring\n",
        "%cd Scripts\n",
        "!activate\n",
        "%cd ..\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upgrade pip with mentioned command below.\n",
        "&\n",
        "\n",
        "Upgrade pip with mentioned command below."
      ],
      "metadata": {
        "id": "mhulhORudq0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bi-1obxdvHA",
        "outputId": "963e5b7c-29e8-4661-ab48-2ef8b929e831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download yolov7 object detection weights from link and move them to the working directory {yolov7-object-blurring}"
      ],
      "metadata": {
        "id": "x1MVq_qleCue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yolov7\n",
        "\n",
        "# Download the weights file\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
        "\n",
        "# Move the file to the working directory\n",
        "!mv yolov7.pt yolov7-object-blurring"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMCn7VkreFoG",
        "outputId": "f50af923-5471-4d9b-e7d2-d4a5c23999e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yolov7 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: yolov5 in /usr/local/lib/python3.10/dist-packages (from yolov7) (7.0.13)\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (3.1.43)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (1.25.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (9.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (1.11.4)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (4.66.4)\n",
            "Requirement already satisfied: ultralytics>=8.0.100 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (8.2.32)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (2.15.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.13.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (67.7.2)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.6.0)\n",
            "Requirement already satisfied: boto3>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (1.34.126)\n",
            "Requirement already satisfied: sahi>=0.11.10 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.11.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (0.23.3)\n",
            "Requirement already satisfied: roboflow>=0.2.29 in /usr/local/lib/python3.10/dist-packages (from yolov5->yolov7) (1.1.32)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.126 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5->yolov7) (1.34.126)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5->yolov7) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5->yolov7) (0.10.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->yolov5->yolov7) (4.0.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->yolov5->yolov7) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->yolov5->yolov7) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->yolov5->yolov7) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->yolov5->yolov7) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (0.10.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5->yolov7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5->yolov7) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5->yolov7) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5->yolov7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5->yolov7) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5->yolov7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5->yolov7) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (4.0.0)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (4.8.0.74)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (1.16.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5->yolov7) (0.4.27)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5->yolov7) (2.0.4)\n",
            "Requirement already satisfied: pybboxes==0.1.6 in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5->yolov7) (0.1.6)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5->yolov7) (3.1.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5->yolov7) (8.1.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5->yolov7) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5->yolov7) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->yolov5->yolov7) (12.5.40)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.100->yolov5->yolov7) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.100->yolov5->yolov7) (0.2.8)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->yolov5->yolov7) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5->yolov7) (5.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5->yolov7) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5->yolov7) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5->yolov7) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5->yolov7) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5->yolov7) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->yolov5->yolov7) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5->yolov7) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5->yolov7) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m--2024-06-14 15:23:14--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240614T152314Z&X-Amz-Expires=300&X-Amz-Signature=c633cd6836280bedf05d79b68ca9512163d386eff92834116a84c5ebeff7cd2a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-06-14 15:23:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240614T152314Z&X-Amz-Expires=300&X-Amz-Signature=c633cd6836280bedf05d79b68ca9512163d386eff92834116a84c5ebeff7cd2a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75587165 (72M) [application/octet-stream]\n",
            "Saving to: ‘yolov7.pt’\n",
            "\n",
            "yolov7.pt           100%[===================>]  72.08M   314MB/s    in 0.2s    \n",
            "\n",
            "2024-06-14 15:23:14 (314 MB/s) - ‘yolov7.pt’ saved [75587165/75587165]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Run the code with mentioned command below.\""
      ],
      "metadata": {
        "id": "ieZQcD8Afctb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://@github.com//repo.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHboZSVu3js",
        "outputId": "e6c8b58c-c044-4494-83d3-7ef35b72aedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement document_base (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for document_base\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Object Crop Using YOLOv7\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "#appended\n",
        "import sys\n",
        "print(sys.path)\n",
        "\n",
        "\n",
        "from doqu.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "TNG1w_KtjE9-",
        "outputId": "33eefad8-a6de-42d1-8283-73a8b07e0d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'document_base'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cef31067c625>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdoqu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadStreams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_img_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_requirements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_imshow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/doqu/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#    along with Doqu.  If not, see <http://gnu.org/licenses/>.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdocument_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMany\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_fixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'document_base'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 of"
      ],
      "metadata": {
        "id": "aeb0QEhUjWid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect(save_img=False):\n",
        "    source, weights, view_img, save_txt, imgsz, trace, blurratio,hidedetarea = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace, opt.blurratio,opt.hidedetarea\n",
        "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "\n",
        "    # Directories\n",
        "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Initialize\n",
        "    set_logging()\n",
        "    device = select_device(opt.device)\n",
        "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "    stride = int(model.stride.max())  # model stride\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "\n",
        "    if trace:\n",
        "        model = TracedModel(model, device, opt.img_size)\n",
        "\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    # Second-stage classifier\n",
        "    classify = False\n",
        "    if classify:\n",
        "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
        "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "    # Set Dataloader\n",
        "    vid_path, vid_writer = None, None\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
        "\n",
        "    # Get names and colors\n",
        "    names = model.module.names if hasattr(model, 'module') else model.names\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "\n",
        "    # Run inference\n",
        "    if device.type != 'cpu':\n",
        "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "    old_img_w = old_img_h = imgsz\n",
        "    old_img_b = 1\n",
        "\n",
        "    t0 = time.time()\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Warmup\n",
        "        if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
        "            old_img_b = img.shape[0]\n",
        "            old_img_h = img.shape[2]\n",
        "            old_img_w = img.shape[3]\n",
        "            for i in range(3):\n",
        "                model(img, augment=opt.augment)[0]\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img, augment=opt.augment)[0]\n",
        "        t2 = time_synchronized()\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
        "        t3 = time_synchronized()\n",
        "\n",
        "        # Apply Classifier\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
        "            else:\n",
        "                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # img.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "                    #Add Object Blurring Code\n",
        "                    #..................................................................\n",
        "                    crop_obj = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])]\n",
        "                    blur = cv2.blur(crop_obj,(blurratio,blurratio))\n",
        "                    im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])] = blur\n",
        "                    #..................................................................\n",
        "\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or view_img:  # Add bbox to image\n",
        "                        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "                        if not hidedetarea:\n",
        "                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "            print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                    print(f\" The image with the result is saved in: {save_path}\")\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path != save_path:  # new video\n",
        "                        vid_path = save_path\n",
        "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
        "                            vid_writer.release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer.write(im0)\n",
        "\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        #print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "    print(f'Done. ({time.time() - t0:.3f}s)')"
      ],
      "metadata": {
        "id": "Ids76EOpjVyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 of"
      ],
      "metadata": {
        "id": "EDFshenDjo9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L2vsHOAPjVjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Object Crop Using YOLOv7\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "#appended\n",
        "import sys\n",
        "print(sys.path)\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "\n",
        "def detect(save_img=False):\n",
        "    source, weights, view_img, save_txt, imgsz, trace, blurratio,hidedetarea = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace, opt.blurratio,opt.hidedetarea\n",
        "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "\n",
        "    # Directories\n",
        "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Initialize\n",
        "    set_logging()\n",
        "    device = select_device(opt.device)\n",
        "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "    stride = int(model.stride.max())  # model stride\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "\n",
        "    if trace:\n",
        "        model = TracedModel(model, device, opt.img_size)\n",
        "\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    # Second-stage classifier\n",
        "    classify = False\n",
        "    if classify:\n",
        "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
        "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "    # Set Dataloader\n",
        "    vid_path, vid_writer = None, None\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
        "\n",
        "    # Get names and colors\n",
        "    names = model.module.names if hasattr(model, 'module') else model.names\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "\n",
        "    # Run inference\n",
        "    if device.type != 'cpu':\n",
        "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "    old_img_w = old_img_h = imgsz\n",
        "    old_img_b = 1\n",
        "\n",
        "    t0 = time.time()\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Warmup\n",
        "        if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
        "            old_img_b = img.shape[0]\n",
        "            old_img_h = img.shape[2]\n",
        "            old_img_w = img.shape[3]\n",
        "            for i in range(3):\n",
        "                model(img, augment=opt.augment)[0]\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img, augment=opt.augment)[0]\n",
        "        t2 = time_synchronized()\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
        "        t3 = time_synchronized()\n",
        "\n",
        "        # Apply Classifier\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
        "            else:\n",
        "                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # img.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "                    #Add Object Blurring Code\n",
        "                    #..................................................................\n",
        "                    crop_obj = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])]\n",
        "                    blur = cv2.blur(crop_obj,(blurratio,blurratio))\n",
        "                    im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])] = blur\n",
        "                    #..................................................................\n",
        "\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or view_img:  # Add bbox to image\n",
        "                        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "                        if not hidedetarea:\n",
        "                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "            print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                    print(f\" The image with the result is saved in: {save_path}\")\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path != save_path:  # new video\n",
        "                        vid_path = save_path\n",
        "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
        "                            vid_writer.release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer.write(im0)\n",
        "\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        #print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "    print(f'Done. ({time.time() - t0:.3f}s)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='yolov7.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--source', type=str, default='inference/images', help='source')  # file/folder, 0 for webcam\n",
        "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='display results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--no-trace', action='store_true', help='don`t trace model')\n",
        "    parser.add_argument('--blurratio',type=int,default=30, required=True, help='blur opacity')\n",
        "    parser.add_argument('--hidedetarea',action='store_true', help='Hide Detected Area')\n",
        "    opt = parser.parse_args()\n",
        "    print(opt)\n",
        "    #check_requirements(exclude=('pycocotools', 'thop'))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if opt.update:  # update all models (to fix SourceChangeWarning)\n",
        "            for opt.weights in ['yolov7.pt']:\n",
        "                detect()\n",
        "                strip_optimizer(opt.weights)\n",
        "        else:\n",
        "            detect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "exdpcA0RfYt2",
        "outputId": "058928da-9be3-41b0-da7a-7203cb54be77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8c03a4218751>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadStreams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_img_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_requirements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_imshow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}